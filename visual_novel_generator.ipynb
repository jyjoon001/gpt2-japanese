{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "visual-novel-generator.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPTcoVhJwe6IUkjjDBv+QmC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyjoon001/gpt2-japanese/blob/master/visual_novel_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6Fxv4WTRrJ2"
      },
      "source": [
        "# Visual Novel Generator\n",
        "\n",
        "이 GPT-2 모델은 사전 학습된 일어 GPT-2 모델에 일본어 텍스트 어드벤처 게임에서 추출된 약 20만줄의 정제된 텍스트 데이터를 전이학습하여 Fine-Tuning하였습니다. \n",
        "\n",
        "\n",
        "Pre-Trained Model과 Training을 위한 기반 코드는 [Japanese GPT2 Generation Model](https://github.com/tanreinama/gpt2-japanese) 을 사용하였습니다. \n",
        "> [@jyjoon001](https://github.com/jyjoon001) , forked from [tanreinama.](https://github.com/tanreinama) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-u_0TRhRqnj",
        "outputId": "52bc6bcb-8b07-4963-fbf1-1d37ec12970f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HipI56hcYk_y"
      },
      "source": [
        "Google Drive에 파일이 있는지 확인하며 파일이 존재하지 않을 경우 Fine-Tune된 모델과 코드를 다운로드합니다. <br> 해당 파일은 1.1GB의 크기를 가지고 있습니다. \n",
        "* 적절한 웹호스팅 공간이 없어 임시로 Naver Mail 서버를 사용하고 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "IpNYfLjCXFf9",
        "outputId": "889bb5f8-0483-4929-a06f-6f41070f0478"
      },
      "source": [
        "import os\n",
        "if os.path.isfile('/content/drive/MyDrive/visual-novel-generator/gpt2-generate.py') is True:\n",
        "    print('path found.')\n",
        "else:\n",
        "    print('path not found, downloading model.')\n",
        "    !wget https://bigfile.mail.naver.com/bigfileupload/download?fid=pXR5Wre8D6cZHqujKxgmFoMdax2mHqUmKoumFqg9KxbwFqEdHqurKo2maAvwaxvjF634KzFoMqg9FqUwp6EmF6trF4EZpxFoK6EdFAK9Mov=\n",
        "    !unzip download?fid=pXR5Wre8D6cZHqujKxgmFoMdax2mHqUmKoumFqg9KxbwFqEdHqurKo2maAvwaxvjF634KzFoMqg9FqUwp6EmF6trF4EZpxFoK6EdFAK9Mov="
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-77c780f77d5f>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    os.makedirs(os.path.join(/content/drive/MyDrive/, 'visual-novel-generator'))\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4NgjI4hg3r2"
      },
      "source": [
        "starting_word를 조정해서 생성 모델이 어떤 후속 문장을 생성할지를 정할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmg5VRvUXM78"
      },
      "source": [
        "starting_word = \"けれど、あなたは私に夢と希望をくれました。\" #하지만 당신은 저에게 꿈과 희망을 주었습니다."
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW6XGhPhhKeu"
      },
      "source": [
        "필요한 코드를 실행시키기 위하여 필요한 라이브러리를 다운받습니다. \n",
        "</br> jaconv는 정제되지 않은 일어 데이터를 unicode 데이터로 normalize합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZalnoUJXRzj",
        "outputId": "59331101-06d4-435a-a985-c1715c5585c1"
      },
      "source": [
        "%cd /content/drive/MyDrive/visual-novel-generator/\n",
        "with open(\"prologue.txt\", \"w\") as output:\n",
        "     output.write(starting_word)\n",
        "!pip install jaconv"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/visual-novel-generator/'\n",
            "/content/drive/MyDrive\n",
            "Requirement already satisfied: jaconv in /usr/local/lib/python3.7/dist-packages (0.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cQh4bnsg5Zl"
      },
      "source": [
        "gpt2-generate.py를 통하여 소스 코드를 실행시킵니다. \n",
        "해당 gpt2-generate.py는 다음과 같은 형태로 이루어져 있습니다. \n",
        "* argparse를 통하여 argument를 설정합니다. 사용하는 model, 입력/출력 문장, 최대/최소 음절과 문장이 얼마나 임의의 단어를 생성하는지에 대한 temperature 계수, top-k 및 top-p sampling 시 사용하는 어절의 갯수 등을 결정합니다. \n",
        "* 사용하는 Model의 정보를 불러온 뒤 layer 갯수 등의 정보를 결정합니다. \n",
        "* generate_one 함수에서는 우선 입력 문장을 encode하여  Tensorflow session을 실행시켜 후속 토큰을 생성하며, 이를 decode하여 자연어로 출력한다. 이 때```'<|endoftext|>'``` 토큰이 나올 경우는 생성된 문장에 last 라벨링을 하여 문장의 끝을 알 수 있게 합니다. \n",
        "* Tensorflow session의 경우 sampling.py의 sample_sequence를 통해서 output을 결정합니다. 이후, max_length까지 위의 함수를 통하여 문장을 generate합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBiex_g0XSxy"
      },
      "source": [
        "!python3 gpt2-generate.py --num_generate 1 --temperature 1.5 --max_length 1024 --min_length 512\n",
        "from google.colab import files\n",
        "files.download(\"script.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}